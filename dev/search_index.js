var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#General-Calibration-Error","page":"API","title":"General Calibration Error","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ConsistencyTest","category":"page"},{"location":"api/#CalibrationTests.ConsistencyTest","page":"API","title":"CalibrationTests.ConsistencyTest","text":"ConsistencyTest(\n    estimator::CalibrationErrorEstimator,\n    predictions::AbstractVector,\n    targets::AbstractVector,\n)\n\nConstruct an hypothesis test of calibration based on consistency resampling with a calibration estimator as test statistic and predictions and targets of a model of interest.\n\nConsistency resampling is a parametric bootstrap method for calibrated models.\n\nReferences\n\nBr√∂cker, J., & Smith, L. A. (2007). Increasing the reliability of reliability diagrams. Weather and forecasting, 22(3), 651-661.\n\n\n\n\n\n","category":"type"},{"location":"api/#Kernel-Calibration-Error","page":"API","title":"Kernel Calibration Error","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"AsymptoticSKCETest","category":"page"},{"location":"api/#CalibrationTests.AsymptoticSKCETest","page":"API","title":"CalibrationTests.AsymptoticSKCETest","text":"AsymptoticSKCETest(kernel::Kernel, predictions, targets)\n\nCalibration hypothesis test based on the unbiased estimator of the squared kernel calibration error (SKCE) with quadratic sample complexity.\n\nDetails\n\nLet mathcalD = (P_X_i Y_i)_i=1ldotsn be a data set of predictions and corresponding targets. Denote the null hypothesis \"the predictive probabilistic model is calibrated\" with H_0.\n\nThe hypothesis test approximates the p-value ‚Ñô(mathrmSKCE_uq  c  H_0), where mathrmSKCE_uq is the unbiased estimator of the SKCE, defined as\n\nfrac2n(n-1) sum_1 leq i  j leq n h_kbig((P_X_i Y_i) (P_X_j Y_j)big)\n\nwhere\n\nbeginaligned\nh_kbig((Œº y) (Œº y)big) =   kbig((Œº y) (Œº y)big)\n                                   - ùîº_Z  Œº kbig((Œº Z) (Œº y)big) \n                                  - ùîº_Z  Œº kbig((Œº y) (Œº Z)big)\n                                   + ùîº_Z  Œº Z  Œº kbig((Œº Z) (Œº Z)big)\nendaligned\n\nThe p-value is estimated based on the asymptotically valid approximation\n\n‚Ñô(nmathrmSKCE_uq  c  H_0) approx ‚Ñô(T  c  mathcalD)\n\nwhere T is the bootstrap statistic\n\nT = frac2n sum_1 leq i  j leq n bigg(h_kbig((P^*_X_i Y^*_i) (P^*_X_j Y^*_j)big)\n- frac1n sum_r = 1^n h_kbig((P^*_X_i Y^*_i) (P_X_r Y_r)big)\n- frac1n sum_r = 1^n h_kbig((P_X_r Y_r) (P^*_X_j Y^*_j)big)\n+ frac1n^2 sum_r s = 1^n h_kbig((P_X_r Y_r) (P_X_s Y_s)big)bigg)\n\nfor bootstrap samples (P^*_X_i Y^*_i)_i=1ldotsn of mathcalD. This can be reformulated to the approximation\n\n‚Ñô(nmathrmSKCE_uq(n - 1) - mathrmSKCE_b  c  H_0) approx ‚Ñô(T  c  mathcalD)\n\nwhere\n\nmathrmSKCE_b = frac1n^2 sum_i j = 1^n h_kbig((P_X_i Y_i) (P_X_j Y_j)big)\n\nand\n\nT = frac2n(n - 1) sum_1 leq i  j leq n h_kbig((P^*_X_i Y^*_i) (P^*_X_j Y^*_j)big)\n- frac2n^2 sum_i r=1^n h_kbig((P^*_X_i Y^*_i) (P_X_r Y_r)big)\n\nReferences\n\nWidmann, D., Lindsten, F., & Zachariah, D. (2019). Calibration tests in multi-class classification: A unifying framework. In: Advances in Neural Information Processing Systems (NeurIPS 2019) (pp. 12257‚Äì12267).\n\nWidmann, D., Lindsten, F., & Zachariah, D. (2021). Calibration tests beyond classification.\n\n\n\n\n\n","category":"type"},{"location":"#CalibrationTests.jl","page":"Home","title":"CalibrationTests.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Hypothesis tests of calibration.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package implements different hypothesis tests for calibration of probabilistic models in the Julia language.","category":"page"},{"location":"#Citing","page":"Home","title":"Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use CalibrationTests.jl as part of your research, teaching, or other activities, please consider citing the following publications:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Widmann, D., Lindsten, F., & Zachariah, D. (2019). Calibration tests in multi-class classification: A unifying framework. In Advances in Neural Information Processing Systems 32 (NeurIPS 2019) (pp. 12257‚Äì12267).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Widmann, D., Lindsten, F., & Zachariah, D. (2021). Calibration tests beyond classification. International Conference on Learning Representations (ICLR 2021).","category":"page"},{"location":"#Acknowledgements","page":"Home","title":"Acknowledgements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This work was financially supported by the Swedish Research Council via the projects Learning of Large-Scale Probabilistic Dynamical Models (contract number: 2016-04278), Counterfactual Prediction Methods for Heterogeneous Populations (contract number: 2018-05040), and Handling Uncertainty in Machine Learning Systems (contract number: 2020-04122), by the Swedish Foundation for Strategic Research via the project Probabilistic Modeling and Inference for Machine Learning (contract number: ICA16-0015), by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation, and by ELLIIT.","category":"page"}]
}
